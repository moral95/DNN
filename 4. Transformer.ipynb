{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Overview]\n",
    "\n",
    "1. 라이브러리 설정\n",
    "2. HyperParameter 설정\n",
    "3. GPU 설정\n",
    "4. Data 준비(Data 불러오기, Data 전처리, Tokenizing)\n",
    "5. 모델 설계\n",
    "6. 모델 훈련 (Training)\n",
    "7. 모델 검증 (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 라이브러리 설정\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch \n",
    "import sentencepiece as spm\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Transformer\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3936, 0.5584, 0.9692]])\n"
     ]
    }
   ],
   "source": [
    "# 2. HyperParameter 설정\n",
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 64\n",
    "lr = 1e-4\n",
    "embed_size = 256 \n",
    "n_head=8\n",
    "n_hid = 512\n",
    "n_layer = 2\n",
    "dropout = 0.1\n",
    "epoch = 30\n",
    "\n",
    "#seed\n",
    "random_seed = 9712\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.enabled = False\n",
    "print(torch.randn(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GPU 설정\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data 설정\n",
    "train_data = pd.read_csv('https://raw.githubusercontent.com/Doheon/Chatbot-Transformer/main/ChatBotData.csv')\n",
    "\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []    \n",
    "for sentence in train_data['A']:\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "with open('all.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(questions))\n",
    "    f.write('\\n'.join(answers))\n",
    "# Q & A 리스트를 만들고\n",
    "# ???///. Train_data, Test_data 안에 'Q','A'인데 왜 [Q]으로 담아져 있나?? -> 사용 데이터를 보면 Q, A로 구분되어 있으니, 그렇게 구분하였다.\n"
    "# re.sub로 불필요한 것을 제거-> sub 제거.\n",
    "# ???. strip\n",
    "# questions의 빈 리스트 안에 sentence를 집어넣는다.\n",
    "# ???.  with open 기능을 사용하는 법은 모르겠음.\n",
    "\n",
    "corpus = \"all.txt\"\n",
    "prefix = \"chatbot\"\n",
    "vocab_size = 16000\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +\n",
    "    \"--min_frequency={3}\"+\n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰    \n",
    "# spm.SentencePieceTrainer.train 기능을 봐야겠지만..\n",
    "# ???. --input이 어떻게 작동하는 지 모르겠음\n",
    "\n",
    "vocab_file = \"chatbot.model\"\n",
    "vocab = spm.SentencePieceProcessor().to(device)\n",
    "vocab.load(vocab_file)\n",
    "line = \"안녕하세요 만나서 반갑습니다\"\n",
    "pieces = vocab.encode_as_pieces(line).to(device)\n",
    "ids = vocab.encode_as_ids(line).to(device)\n",
    "\n",
    "print(line)\n",
    "print(pieces)\n",
    "print(ids)\n",
    "print(vocab.GetPieceSize())\n",
    "vocab_size = vocab.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = [2]\n",
    "END_TOKEN = [3]\n",
    "# 왜 이렇게 주는지??\n",
    "\n",
    "#tokenize and padding  \n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    zeros1 = np.zeros(MAX_LENGTH, dtype=int)\n",
    "    zeros2 = np.zeros(MAX_LENGTH, dtype=int)\n",
    "    sentence1 = START_TOKEN + vocab.encode_as_ids(sentence1) + END_TOKEN\n",
    "    zeros1[:len(sentence1)] = sentence1[:MAX_LENGTH]\n",
    "\n",
    "    sentence2 = START_TOKEN + vocab.encode_as_ids(sentence2) + END_TOKEN\n",
    "    zeros2[:len(sentence2)] = sentence2[:MAX_LENGTH]\n",
    "\n",
    "    tokenized_inputs.append(zeros1)\n",
    "    tokenized_outputs.append(zeros2)\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions_encode, answers_encode = tokenize_and_filter(questions, answers)\n",
    "print(questions_encode[0])\n",
    "print(answers_encode[0])\n",
    "# inputs, outputs에 [] 형식으로 담는다.\n",
    "# zip(inputs, ouputs) 안에 반목문 임의의 sentence1, 2를 실행\n",
    "# ???. zeros에 maxLength 담는다.\n",
    "# vocab = SenctencePiceProcessor 내 encode_as_ids의 기능을 사용해서 안에 담아둔다.\n",
    "# 넘파이 zeros, Max_Length 에 정리된 sentence에 max length을 지정\n",
    "# tokenize_and_filter내에 q,a를 넣어 변수를 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, questions, answers):\n",
    "        questions = np.array(questions)\n",
    "        answers = np.array(answers)\n",
    "        self.inputs = questions\n",
    "        self.dec_inputs = answers[:,:-1] #(datanum, max_len-1)\n",
    "        self.outputs = answers[:,1:] #(datanum, max_len-1) \n",
    "        self.length = len(questions) #input_length \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.inputs[idx], self.dec_inputs[idx], self.outputs[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "dataset = MyDataset(questions_encode, answers_encode)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "print(f\"data set num: {len(dataset)}\")\n",
    "print(f\"data set num: {len(dataloader)}\")\n",
    "# class 내에 있는 datasets 구성 요소와 return하여 나온 변수들이 어떻게 사용되는 지 잘 모르겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_head, n_hid, n_layer, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = Transformer(embed_size, n_head, dim_feedforward=n_hid, num_encoder_layers=n_layer, num_decoder_layers=n_layer,dropout=dropout)\n",
    "        self.e_pos = PositionalEncoding(embed_size, dropout)\n",
    "        self.e_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.d_pos = PositionalEncoding(embed_size, dropout)\n",
    "        self.encoder_d = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.e_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt, srcmask, tgtmask, srcpadmask, tgtpadmask):\n",
    "        src = self.e_embedding(src) * math.sqrt(self.embed_size) #(batch_size, max_len, embed_size)\n",
    "        src = self.e_pos(src) #(batch_size, max_len, embed_size)\n",
    "        tgt = self.encoder_d(tgt) * math.sqrt(self.embed_size)#(batch_size, max_len-1, embed_size)\n",
    "        tgt = self.d_pos(tgt)#(batch_size, max_len-1, embed_size)\n",
    "        output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask, src_key_padding_mask=srcpadmask, tgt_key_padding_mask=tgtpadmask) #(max_len-1, batch_size,embed_size)\n",
    "        output = self.linear(output) #???\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def gen_attention_mask(x):\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask\n",
    "    \n",
    "model = TransformerModel(vocab_size, embed_size=embed_size, n_head=n_head, n_hid=n_hid, n_layer=n_layer, dropout=dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for i in range(epoch):\n",
    "    batchloss = 0.0\n",
    "    progress = tqdm(dataloader)\n",
    "    for (inputs, dec_inputs, outputs) in progress:\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).to(device) #(max_len, max_len)\n",
    "        src_padding_mask = gen_attention_mask(inputs).to(device) #(batch_size, max_len)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).to(device)  #(max_len-1, max_len-1)\n",
    "        tgt_padding_mask = gen_attention_mask(dec_inputs).to(device) #(batch_size, max_len-1)\n",
    "        result = model(inputs.to(device), dec_inputs.to(device), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask) #(max_len, batch_size, vocab_size)\n",
    "        loss = criterion(result.permute(1,2,0), outputs.to(device).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batchloss += loss\n",
    "        progress.set_description(\"{:0.3f}\".format(loss))\n",
    "    print(\"epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(dataloader))\n",
    "# ???. mask 방법, padding_mask 이해 하려면 모델을 다시 봐야 이해 할 수 있을 것 같음.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"chatbot.pth\")\n",
    "# ???. state_dict()은 무엇?. pth의 형태로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "# 불필요한 내용 정리. \n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input = torch.tensor([START_TOKEN + vocab.encode_as_ids(sentence) + END_TOKEN]).to(device)\n",
    "    output = torch.tensor([START_TOKEN]).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # ???. 왜 range를 Max_Length?..\n",
    "        #mask \n",
    "        src_mask = model.generate_square_subsequent_mask(input.shape[1]).to(device)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(output.shape[1]).to(device)\n",
    "        src_padding_mask = gen_attention_mask(input).to(device)\n",
    "        tgt_padding_mask = gen_attention_mask(output).to(device)\n",
    "\n",
    "        predictions = model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1) #(batch_size, output_size, vocab_size)\n",
    "\n",
    "        # ???. 현재(마지막) 시점의 예측 단어를 받아온다.???\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = torch.LongTensor(torch.argmax(predictions.cpu(), axis=-1))\n",
    "\n",
    "\n",
    "        # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if torch.equal(predicted_id[0][0], torch.tensor(END_TOKEN[0])):\n",
    "            break\n",
    "\n",
    "        # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "        # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "        output = torch.cat([output, predicted_id.to(device)], axis=1)\n",
    "\n",
    "    return torch.squeeze(output, axis=0).cpu().numpy()\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size])))\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    print('Q: {}'.format(sentence))\n",
    "    print('A: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"chatbot.pth\"))\n",
    "result = predict(\"놀고싶다\")\n",
    "result = predict(\"감기 같애\")\n",
    "result = predict(\"건강하게 다이어트 하는 방법\")\n",
    "result = predict(\"게임하고 싶어\")\n",
    "result = predict(\"궁금하지?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
